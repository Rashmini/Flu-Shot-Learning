{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Ensemble_H1N1_xgb_lgb.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEt-dh0t1NOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrvCab301JOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Suppress warnings \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_SEED = 42    # Set a random seed for reproducibility!\n",
        "\n",
        "pd.set_option('display.max_columns', 100)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUo1fFru1ebB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e071d157-2ad1-4ffb-f742-db9296fdb7e2"
      },
      "source": [
        "cd \"/content/drive/My Drive/DM\""
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pnN850U1ksV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = Path.cwd().parent / \"data\" / \"final\" / \"public\""
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkGn8uj1JON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_df = pd.read_csv(\n",
        "    DATA_PATH / \"/content/drive/My Drive/DM/training_set_features.csv\", \n",
        "    index_col=\"respondent_id\"\n",
        ")\n",
        "labels_df = pd.read_csv(\n",
        "    DATA_PATH / \"/content/drive/My Drive/DM/training_set_labels.csv\", \n",
        "    index_col=\"respondent_id\"\n",
        ")\n",
        "test_features_df = pd.read_csv(\n",
        "    DATA_PATH / \"/content/drive/My Drive/DM/test_set_features.csv\", \n",
        "    index_col=\"respondent_id\"\n",
        ")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pgK9nus1JOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5d7e87c6-2b5f-4be8-d63b-d9f51dbd96c0"
      },
      "source": [
        "print('features_df' , features_df.shape)\n",
        "print('labels_df', labels_df.shape)\n",
        "print('test_features_df' , test_features_df.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features_df (26707, 35)\n",
            "labels_df (26707, 2)\n",
            "test_features_df (26708, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_BabCTL1JOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_df_h1n1 = labels_df[['h1n1_vaccine']]\n",
        "labels_df_seasonal = labels_df[['seasonal_vaccine']]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od7TV0Bd1JOY",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtvKV97L1JOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_df = features_df.drop([ 'household_children'], axis = 1)\n",
        "test_features_df = test_features_df.drop([ 'household_children'], axis = 1)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvO4bE561JOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_cols = features_df.columns[features_df.dtypes != 'object'].values\n",
        "non_numeric_cols = features_df.columns[features_df.dtypes == 'object'].values"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM5RCJjX1JOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_preprocessing_steps = Pipeline([\n",
        "    ('standard_scaler', StandardScaler()),\n",
        "    ('simple_imputer', SimpleImputer(strategy = 'mean'))\n",
        "])\n",
        "\n",
        "non_numeric_preprocessing_steps = Pipeline([\n",
        "     ('simple_imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('one_hot_encoder', OneHotEncoder())\n",
        "])\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers = [\n",
        "        ('numeric', numeric_preprocessing_steps, numeric_cols),\n",
        "        ('non_numeric', non_numeric_preprocessing_steps, non_numeric_cols)  \n",
        "    ],\n",
        "    remainder = \"drop\"\n",
        ")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-jnDhI1JOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_eval, y_train, y_eval = train_test_split(\n",
        "    features_df,\n",
        "    labels_df_h1n1,\n",
        "    test_size=0.33,\n",
        "    shuffle=True,\n",
        "    stratify=labels_df_h1n1,\n",
        "    random_state=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtY2jOmH1JOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ecb52cf5-fb01-4686-e397-c33900a303b6"
      },
      "source": [
        "X_train_preprocess = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
        "X_eval_preprocess =  pd.DataFrame(preprocessor.transform(X_eval))\n",
        "print ('X_train_preprocess.shape' , X_train_preprocess.shape)\n",
        "print ('X_eval_preprocess.shape' , X_eval_preprocess.shape)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_preprocess.shape (17893, 111)\n",
            "X_eval_preprocess.shape (8814, 111)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APgIB2891JOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9635fe9f-a79d-4f4e-a40c-ef87c680b06d"
      },
      "source": [
        "features_df_preprocess = pd.DataFrame(preprocessor.fit_transform(features_df))\n",
        "test_features_df_preprocess = pd.DataFrame(preprocessor.transform(test_features_df))\n",
        "print ('features_df_preprlabels_df.ocess.shape' , features_df_preprocess.shape)\n",
        "print ('test_features_df_preprocess.shape' , test_features_df_preprocess.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features_df_preprlabels_df.ocess.shape (26707, 111)\n",
            "test_features_df_preprocess.shape (26708, 111)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNOjjxJS1JOs",
        "colab_type": "text"
      },
      "source": [
        "# Remove Collinear Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD_g3UlM1JOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "c8c602c5-5f7c-4f35-f588-89388d3a2fa4"
      },
      "source": [
        "#Threshold for removing correlated variables\n",
        "threshold = 0.9\n",
        "\n",
        "# Absolute value correlation matrix\n",
        "corr_matrix = X_train_preprocess.corr().abs()\n",
        "corr_matrix.head()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>...</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.061255</td>\n",
              "      <td>0.097908</td>\n",
              "      <td>0.235519</td>\n",
              "      <td>0.153747</td>\n",
              "      <td>0.296449</td>\n",
              "      <td>0.250226</td>\n",
              "      <td>0.245242</td>\n",
              "      <td>0.242955</td>\n",
              "      <td>0.143139</td>\n",
              "      <td>0.128204</td>\n",
              "      <td>0.083699</td>\n",
              "      <td>0.046147</td>\n",
              "      <td>0.023825</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>0.239307</td>\n",
              "      <td>0.367953</td>\n",
              "      <td>0.353836</td>\n",
              "      <td>0.229209</td>\n",
              "      <td>0.322467</td>\n",
              "      <td>0.221108</td>\n",
              "      <td>0.012822</td>\n",
              "      <td>0.089840</td>\n",
              "      <td>0.025439</td>\n",
              "      <td>0.028723</td>\n",
              "      <td>0.030126</td>\n",
              "      <td>0.006951</td>\n",
              "      <td>0.030633</td>\n",
              "      <td>0.053219</td>\n",
              "      <td>0.034692</td>\n",
              "      <td>0.022967</td>\n",
              "      <td>0.004044</td>\n",
              "      <td>0.101911</td>\n",
              "      <td>0.072349</td>\n",
              "      <td>0.030513</td>\n",
              "      <td>0.130322</td>\n",
              "      <td>0.131524</td>\n",
              "      <td>0.131524</td>\n",
              "      <td>0.019951</td>\n",
              "      <td>0.022204</td>\n",
              "      <td>0.071412</td>\n",
              "      <td>0.005709</td>\n",
              "      <td>0.025334</td>\n",
              "      <td>0.024106</td>\n",
              "      <td>0.003151</td>\n",
              "      <td>0.001551</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.055620</td>\n",
              "      <td>0.042326</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002191</td>\n",
              "      <td>0.004912</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>0.004651</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>0.011635</td>\n",
              "      <td>0.011033</td>\n",
              "      <td>0.027165</td>\n",
              "      <td>0.040901</td>\n",
              "      <td>0.054317</td>\n",
              "      <td>0.031498</td>\n",
              "      <td>0.010318</td>\n",
              "      <td>0.023400</td>\n",
              "      <td>0.054592</td>\n",
              "      <td>0.026762</td>\n",
              "      <td>0.024159</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.019440</td>\n",
              "      <td>0.012658</td>\n",
              "      <td>0.013749</td>\n",
              "      <td>0.005596</td>\n",
              "      <td>0.017371</td>\n",
              "      <td>0.020168</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.036841</td>\n",
              "      <td>0.003189</td>\n",
              "      <td>0.003768</td>\n",
              "      <td>0.031938</td>\n",
              "      <td>0.019541</td>\n",
              "      <td>0.054317</td>\n",
              "      <td>0.018505</td>\n",
              "      <td>0.006188</td>\n",
              "      <td>0.008820</td>\n",
              "      <td>0.010780</td>\n",
              "      <td>0.007550</td>\n",
              "      <td>0.026797</td>\n",
              "      <td>0.052368</td>\n",
              "      <td>0.002064</td>\n",
              "      <td>0.006276</td>\n",
              "      <td>0.019390</td>\n",
              "      <td>0.017358</td>\n",
              "      <td>0.029372</td>\n",
              "      <td>0.018410</td>\n",
              "      <td>0.018153</td>\n",
              "      <td>0.020319</td>\n",
              "      <td>0.020459</td>\n",
              "      <td>0.023357</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>0.023514</td>\n",
              "      <td>0.012959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.061255</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.019318</td>\n",
              "      <td>0.089261</td>\n",
              "      <td>0.027572</td>\n",
              "      <td>0.091289</td>\n",
              "      <td>0.047968</td>\n",
              "      <td>0.067313</td>\n",
              "      <td>0.086992</td>\n",
              "      <td>0.090355</td>\n",
              "      <td>0.070768</td>\n",
              "      <td>0.019057</td>\n",
              "      <td>0.026587</td>\n",
              "      <td>0.168941</td>\n",
              "      <td>0.098381</td>\n",
              "      <td>0.112882</td>\n",
              "      <td>0.078066</td>\n",
              "      <td>0.020658</td>\n",
              "      <td>0.077578</td>\n",
              "      <td>0.074995</td>\n",
              "      <td>0.058886</td>\n",
              "      <td>0.024087</td>\n",
              "      <td>0.048839</td>\n",
              "      <td>0.048039</td>\n",
              "      <td>0.081268</td>\n",
              "      <td>0.063335</td>\n",
              "      <td>0.127749</td>\n",
              "      <td>0.141786</td>\n",
              "      <td>0.210952</td>\n",
              "      <td>0.261835</td>\n",
              "      <td>0.026563</td>\n",
              "      <td>0.088745</td>\n",
              "      <td>0.114727</td>\n",
              "      <td>0.069249</td>\n",
              "      <td>0.028289</td>\n",
              "      <td>0.135692</td>\n",
              "      <td>0.069205</td>\n",
              "      <td>0.069205</td>\n",
              "      <td>0.003760</td>\n",
              "      <td>0.195859</td>\n",
              "      <td>0.163628</td>\n",
              "      <td>0.100342</td>\n",
              "      <td>0.140062</td>\n",
              "      <td>0.103863</td>\n",
              "      <td>0.082978</td>\n",
              "      <td>0.153963</td>\n",
              "      <td>0.113285</td>\n",
              "      <td>0.088342</td>\n",
              "      <td>0.161453</td>\n",
              "      <td>0.103778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013197</td>\n",
              "      <td>0.058255</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.065270</td>\n",
              "      <td>0.043238</td>\n",
              "      <td>0.003587</td>\n",
              "      <td>0.022077</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>0.170398</td>\n",
              "      <td>0.003705</td>\n",
              "      <td>0.049280</td>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.026029</td>\n",
              "      <td>0.161001</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.014658</td>\n",
              "      <td>0.011647</td>\n",
              "      <td>0.002205</td>\n",
              "      <td>0.009047</td>\n",
              "      <td>0.039806</td>\n",
              "      <td>0.003761</td>\n",
              "      <td>0.004687</td>\n",
              "      <td>0.011070</td>\n",
              "      <td>0.086872</td>\n",
              "      <td>0.039943</td>\n",
              "      <td>0.014223</td>\n",
              "      <td>0.029850</td>\n",
              "      <td>0.018180</td>\n",
              "      <td>0.165063</td>\n",
              "      <td>0.003705</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.071638</td>\n",
              "      <td>0.024342</td>\n",
              "      <td>0.041625</td>\n",
              "      <td>0.032582</td>\n",
              "      <td>0.031485</td>\n",
              "      <td>0.159145</td>\n",
              "      <td>0.028067</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.016649</td>\n",
              "      <td>0.043634</td>\n",
              "      <td>0.015993</td>\n",
              "      <td>0.031861</td>\n",
              "      <td>0.034243</td>\n",
              "      <td>0.039054</td>\n",
              "      <td>0.033432</td>\n",
              "      <td>0.011109</td>\n",
              "      <td>0.029899</td>\n",
              "      <td>0.069826</td>\n",
              "      <td>0.028005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.097908</td>\n",
              "      <td>0.019318</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.056575</td>\n",
              "      <td>0.151058</td>\n",
              "      <td>0.069054</td>\n",
              "      <td>0.112054</td>\n",
              "      <td>0.137627</td>\n",
              "      <td>0.073443</td>\n",
              "      <td>0.047985</td>\n",
              "      <td>0.028460</td>\n",
              "      <td>0.007553</td>\n",
              "      <td>0.026999</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.057040</td>\n",
              "      <td>0.033543</td>\n",
              "      <td>0.112311</td>\n",
              "      <td>0.081772</td>\n",
              "      <td>0.016917</td>\n",
              "      <td>0.091410</td>\n",
              "      <td>0.089189</td>\n",
              "      <td>0.049582</td>\n",
              "      <td>0.067131</td>\n",
              "      <td>0.045152</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>0.038813</td>\n",
              "      <td>0.059568</td>\n",
              "      <td>0.017053</td>\n",
              "      <td>0.062343</td>\n",
              "      <td>0.060574</td>\n",
              "      <td>0.008419</td>\n",
              "      <td>0.036677</td>\n",
              "      <td>0.045850</td>\n",
              "      <td>0.128876</td>\n",
              "      <td>0.019857</td>\n",
              "      <td>0.120744</td>\n",
              "      <td>0.006204</td>\n",
              "      <td>0.006204</td>\n",
              "      <td>0.034006</td>\n",
              "      <td>0.033295</td>\n",
              "      <td>0.091928</td>\n",
              "      <td>0.009372</td>\n",
              "      <td>0.022855</td>\n",
              "      <td>0.008902</td>\n",
              "      <td>0.031475</td>\n",
              "      <td>0.066889</td>\n",
              "      <td>0.051626</td>\n",
              "      <td>0.034587</td>\n",
              "      <td>0.007479</td>\n",
              "      <td>0.018651</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006573</td>\n",
              "      <td>0.025613</td>\n",
              "      <td>0.019998</td>\n",
              "      <td>0.008076</td>\n",
              "      <td>0.017458</td>\n",
              "      <td>0.005947</td>\n",
              "      <td>0.008125</td>\n",
              "      <td>0.003877</td>\n",
              "      <td>0.001638</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.013682</td>\n",
              "      <td>0.013659</td>\n",
              "      <td>0.006706</td>\n",
              "      <td>0.005693</td>\n",
              "      <td>0.015737</td>\n",
              "      <td>0.001489</td>\n",
              "      <td>0.007662</td>\n",
              "      <td>0.005533</td>\n",
              "      <td>0.005553</td>\n",
              "      <td>0.022834</td>\n",
              "      <td>0.001896</td>\n",
              "      <td>0.011442</td>\n",
              "      <td>0.009029</td>\n",
              "      <td>0.009644</td>\n",
              "      <td>0.012161</td>\n",
              "      <td>0.006250</td>\n",
              "      <td>0.008007</td>\n",
              "      <td>0.003962</td>\n",
              "      <td>0.004007</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.003668</td>\n",
              "      <td>0.014339</td>\n",
              "      <td>0.010403</td>\n",
              "      <td>0.009853</td>\n",
              "      <td>0.010688</td>\n",
              "      <td>0.011379</td>\n",
              "      <td>0.008183</td>\n",
              "      <td>0.008879</td>\n",
              "      <td>0.005266</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.025537</td>\n",
              "      <td>0.004010</td>\n",
              "      <td>0.015010</td>\n",
              "      <td>0.019549</td>\n",
              "      <td>0.013936</td>\n",
              "      <td>0.023683</td>\n",
              "      <td>0.000796</td>\n",
              "      <td>0.024786</td>\n",
              "      <td>0.021549</td>\n",
              "      <td>0.004136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.235519</td>\n",
              "      <td>0.089261</td>\n",
              "      <td>0.056575</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.063940</td>\n",
              "      <td>0.340522</td>\n",
              "      <td>0.226385</td>\n",
              "      <td>0.217368</td>\n",
              "      <td>0.333264</td>\n",
              "      <td>0.065109</td>\n",
              "      <td>0.074985</td>\n",
              "      <td>0.036296</td>\n",
              "      <td>0.008610</td>\n",
              "      <td>0.007133</td>\n",
              "      <td>0.025365</td>\n",
              "      <td>0.119277</td>\n",
              "      <td>0.119284</td>\n",
              "      <td>0.133088</td>\n",
              "      <td>0.120487</td>\n",
              "      <td>0.124830</td>\n",
              "      <td>0.087377</td>\n",
              "      <td>0.025328</td>\n",
              "      <td>0.021383</td>\n",
              "      <td>0.012574</td>\n",
              "      <td>0.023982</td>\n",
              "      <td>0.020261</td>\n",
              "      <td>0.031535</td>\n",
              "      <td>0.008608</td>\n",
              "      <td>0.015639</td>\n",
              "      <td>0.015193</td>\n",
              "      <td>0.019528</td>\n",
              "      <td>0.035580</td>\n",
              "      <td>0.003525</td>\n",
              "      <td>0.004542</td>\n",
              "      <td>0.012512</td>\n",
              "      <td>0.002347</td>\n",
              "      <td>0.113015</td>\n",
              "      <td>0.113015</td>\n",
              "      <td>0.012890</td>\n",
              "      <td>0.039126</td>\n",
              "      <td>0.014525</td>\n",
              "      <td>0.051276</td>\n",
              "      <td>0.058036</td>\n",
              "      <td>0.046838</td>\n",
              "      <td>0.025910</td>\n",
              "      <td>0.043006</td>\n",
              "      <td>0.030133</td>\n",
              "      <td>0.027055</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.016326</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003258</td>\n",
              "      <td>0.031249</td>\n",
              "      <td>0.009980</td>\n",
              "      <td>0.024654</td>\n",
              "      <td>0.007197</td>\n",
              "      <td>0.001474</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>0.006061</td>\n",
              "      <td>0.004811</td>\n",
              "      <td>0.040314</td>\n",
              "      <td>0.008067</td>\n",
              "      <td>0.012471</td>\n",
              "      <td>0.007091</td>\n",
              "      <td>0.006854</td>\n",
              "      <td>0.007788</td>\n",
              "      <td>0.015948</td>\n",
              "      <td>0.003497</td>\n",
              "      <td>0.008982</td>\n",
              "      <td>0.009235</td>\n",
              "      <td>0.002548</td>\n",
              "      <td>0.002795</td>\n",
              "      <td>0.007296</td>\n",
              "      <td>0.004714</td>\n",
              "      <td>0.012535</td>\n",
              "      <td>0.024005</td>\n",
              "      <td>0.007406</td>\n",
              "      <td>0.007797</td>\n",
              "      <td>0.010517</td>\n",
              "      <td>0.011064</td>\n",
              "      <td>0.040314</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>0.002812</td>\n",
              "      <td>0.019354</td>\n",
              "      <td>0.006797</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>0.004786</td>\n",
              "      <td>0.028971</td>\n",
              "      <td>0.008780</td>\n",
              "      <td>0.004857</td>\n",
              "      <td>0.020632</td>\n",
              "      <td>0.018439</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.002637</td>\n",
              "      <td>0.022604</td>\n",
              "      <td>0.002541</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.009995</td>\n",
              "      <td>0.011769</td>\n",
              "      <td>0.007664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.153747</td>\n",
              "      <td>0.027572</td>\n",
              "      <td>0.151058</td>\n",
              "      <td>0.063940</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.081527</td>\n",
              "      <td>0.182625</td>\n",
              "      <td>0.162526</td>\n",
              "      <td>0.101801</td>\n",
              "      <td>0.079888</td>\n",
              "      <td>0.069511</td>\n",
              "      <td>0.065472</td>\n",
              "      <td>0.040111</td>\n",
              "      <td>0.072662</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>0.040419</td>\n",
              "      <td>0.125425</td>\n",
              "      <td>0.106117</td>\n",
              "      <td>0.041800</td>\n",
              "      <td>0.106172</td>\n",
              "      <td>0.083387</td>\n",
              "      <td>0.012468</td>\n",
              "      <td>0.006348</td>\n",
              "      <td>0.001255</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>0.008724</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.006807</td>\n",
              "      <td>0.063519</td>\n",
              "      <td>0.037647</td>\n",
              "      <td>0.006854</td>\n",
              "      <td>0.026324</td>\n",
              "      <td>0.046607</td>\n",
              "      <td>0.075540</td>\n",
              "      <td>0.042047</td>\n",
              "      <td>0.102118</td>\n",
              "      <td>0.051000</td>\n",
              "      <td>0.051000</td>\n",
              "      <td>0.020484</td>\n",
              "      <td>0.034493</td>\n",
              "      <td>0.059932</td>\n",
              "      <td>0.018824</td>\n",
              "      <td>0.021212</td>\n",
              "      <td>0.007077</td>\n",
              "      <td>0.031854</td>\n",
              "      <td>0.054471</td>\n",
              "      <td>0.039233</td>\n",
              "      <td>0.032588</td>\n",
              "      <td>0.042068</td>\n",
              "      <td>0.026671</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018491</td>\n",
              "      <td>0.011371</td>\n",
              "      <td>0.030562</td>\n",
              "      <td>0.018673</td>\n",
              "      <td>0.002873</td>\n",
              "      <td>0.021282</td>\n",
              "      <td>0.003303</td>\n",
              "      <td>0.003607</td>\n",
              "      <td>0.070434</td>\n",
              "      <td>0.011893</td>\n",
              "      <td>0.028142</td>\n",
              "      <td>0.009319</td>\n",
              "      <td>0.022141</td>\n",
              "      <td>0.038949</td>\n",
              "      <td>0.018291</td>\n",
              "      <td>0.007058</td>\n",
              "      <td>0.006774</td>\n",
              "      <td>0.029370</td>\n",
              "      <td>0.002861</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>0.013235</td>\n",
              "      <td>0.010980</td>\n",
              "      <td>0.012520</td>\n",
              "      <td>0.027007</td>\n",
              "      <td>0.016464</td>\n",
              "      <td>0.013891</td>\n",
              "      <td>0.011460</td>\n",
              "      <td>0.011491</td>\n",
              "      <td>0.042573</td>\n",
              "      <td>0.011893</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>0.026046</td>\n",
              "      <td>0.033765</td>\n",
              "      <td>0.023007</td>\n",
              "      <td>0.002011</td>\n",
              "      <td>0.022124</td>\n",
              "      <td>0.039410</td>\n",
              "      <td>0.018336</td>\n",
              "      <td>0.018446</td>\n",
              "      <td>0.004055</td>\n",
              "      <td>0.011604</td>\n",
              "      <td>0.021924</td>\n",
              "      <td>0.004945</td>\n",
              "      <td>0.010201</td>\n",
              "      <td>0.006679</td>\n",
              "      <td>0.012990</td>\n",
              "      <td>0.019164</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>0.030686</td>\n",
              "      <td>0.009099</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  1.000000  0.061255  0.097908  0.235519  0.153747  0.296449  0.250226   \n",
              "1  0.061255  1.000000  0.019318  0.089261  0.027572  0.091289  0.047968   \n",
              "2  0.097908  0.019318  1.000000  0.056575  0.151058  0.069054  0.112054   \n",
              "3  0.235519  0.089261  0.056575  1.000000  0.063940  0.340522  0.226385   \n",
              "4  0.153747  0.027572  0.151058  0.063940  1.000000  0.081527  0.182625   \n",
              "\n",
              "        7         8         9         10        11        12        13   \\\n",
              "0  0.245242  0.242955  0.143139  0.128204  0.083699  0.046147  0.023825   \n",
              "1  0.067313  0.086992  0.090355  0.070768  0.019057  0.026587  0.168941   \n",
              "2  0.137627  0.073443  0.047985  0.028460  0.007553  0.026999  0.004883   \n",
              "3  0.217368  0.333264  0.065109  0.074985  0.036296  0.008610  0.007133   \n",
              "4  0.162526  0.101801  0.079888  0.069511  0.065472  0.040111  0.072662   \n",
              "\n",
              "        14        15        16        17        18        19        20   \\\n",
              "0  0.001166  0.239307  0.367953  0.353836  0.229209  0.322467  0.221108   \n",
              "1  0.098381  0.112882  0.078066  0.020658  0.077578  0.074995  0.058886   \n",
              "2  0.057040  0.033543  0.112311  0.081772  0.016917  0.091410  0.089189   \n",
              "3  0.025365  0.119277  0.119284  0.133088  0.120487  0.124830  0.087377   \n",
              "4  0.028800  0.040419  0.125425  0.106117  0.041800  0.106172  0.083387   \n",
              "\n",
              "        21        22        23        24        25        26        27   \\\n",
              "0  0.012822  0.089840  0.025439  0.028723  0.030126  0.006951  0.030633   \n",
              "1  0.024087  0.048839  0.048039  0.081268  0.063335  0.127749  0.141786   \n",
              "2  0.049582  0.067131  0.045152  0.001482  0.038813  0.059568  0.017053   \n",
              "3  0.025328  0.021383  0.012574  0.023982  0.020261  0.031535  0.008608   \n",
              "4  0.012468  0.006348  0.001255  0.002099  0.008724  0.000500  0.006807   \n",
              "\n",
              "        28        29        30        31        32        33        34   \\\n",
              "0  0.053219  0.034692  0.022967  0.004044  0.101911  0.072349  0.030513   \n",
              "1  0.210952  0.261835  0.026563  0.088745  0.114727  0.069249  0.028289   \n",
              "2  0.062343  0.060574  0.008419  0.036677  0.045850  0.128876  0.019857   \n",
              "3  0.015639  0.015193  0.019528  0.035580  0.003525  0.004542  0.012512   \n",
              "4  0.063519  0.037647  0.006854  0.026324  0.046607  0.075540  0.042047   \n",
              "\n",
              "        35        36        37        38        39        40        41   \\\n",
              "0  0.130322  0.131524  0.131524  0.019951  0.022204  0.071412  0.005709   \n",
              "1  0.135692  0.069205  0.069205  0.003760  0.195859  0.163628  0.100342   \n",
              "2  0.120744  0.006204  0.006204  0.034006  0.033295  0.091928  0.009372   \n",
              "3  0.002347  0.113015  0.113015  0.012890  0.039126  0.014525  0.051276   \n",
              "4  0.102118  0.051000  0.051000  0.020484  0.034493  0.059932  0.018824   \n",
              "\n",
              "        42        43        44        45        46        47        48   \\\n",
              "0  0.025334  0.024106  0.003151  0.001551  0.001322  0.000605  0.055620   \n",
              "1  0.140062  0.103863  0.082978  0.153963  0.113285  0.088342  0.161453   \n",
              "2  0.022855  0.008902  0.031475  0.066889  0.051626  0.034587  0.007479   \n",
              "3  0.058036  0.046838  0.025910  0.043006  0.030133  0.027055  0.005711   \n",
              "4  0.021212  0.007077  0.031854  0.054471  0.039233  0.032588  0.042068   \n",
              "\n",
              "        49   ...       61        62        63        64        65        66   \\\n",
              "0  0.042326  ...  0.002191  0.004912  0.000813  0.004651  0.006200  0.011635   \n",
              "1  0.103778  ...  0.013197  0.058255  0.000207  0.065270  0.043238  0.003587   \n",
              "2  0.018651  ...  0.006573  0.025613  0.019998  0.008076  0.017458  0.005947   \n",
              "3  0.016326  ...  0.003258  0.031249  0.009980  0.024654  0.007197  0.001474   \n",
              "4  0.026671  ...  0.018491  0.011371  0.030562  0.018673  0.002873  0.021282   \n",
              "\n",
              "        67        68        69        70        71        72        73   \\\n",
              "0  0.011033  0.027165  0.040901  0.054317  0.031498  0.010318  0.023400   \n",
              "1  0.022077  0.001704  0.170398  0.003705  0.049280  0.000932  0.026029   \n",
              "2  0.008125  0.003877  0.001638  0.000080  0.013682  0.013659  0.006706   \n",
              "3  0.003653  0.006061  0.004811  0.040314  0.008067  0.012471  0.007091   \n",
              "4  0.003303  0.003607  0.070434  0.011893  0.028142  0.009319  0.022141   \n",
              "\n",
              "        74        75        76        77        78        79        80   \\\n",
              "0  0.054592  0.026762  0.024159  0.001698  0.019440  0.012658  0.013749   \n",
              "1  0.161001  0.001203  0.014658  0.011647  0.002205  0.009047  0.039806   \n",
              "2  0.005693  0.015737  0.001489  0.007662  0.005533  0.005553  0.022834   \n",
              "3  0.006854  0.007788  0.015948  0.003497  0.008982  0.009235  0.002548   \n",
              "4  0.038949  0.018291  0.007058  0.006774  0.029370  0.002861  0.000376   \n",
              "\n",
              "        81        82        83        84        85        86        87   \\\n",
              "0  0.005596  0.017371  0.020168  0.001854  0.036841  0.003189  0.003768   \n",
              "1  0.003761  0.004687  0.011070  0.086872  0.039943  0.014223  0.029850   \n",
              "2  0.001896  0.011442  0.009029  0.009644  0.012161  0.006250  0.008007   \n",
              "3  0.002795  0.007296  0.004714  0.012535  0.024005  0.007406  0.007797   \n",
              "4  0.013235  0.010980  0.012520  0.027007  0.016464  0.013891  0.011460   \n",
              "\n",
              "        88        89        90        91        92        93        94   \\\n",
              "0  0.031938  0.019541  0.054317  0.018505  0.006188  0.008820  0.010780   \n",
              "1  0.018180  0.165063  0.003705  0.000323  0.071638  0.024342  0.041625   \n",
              "2  0.003962  0.004007  0.000080  0.003668  0.014339  0.010403  0.009853   \n",
              "3  0.010517  0.011064  0.040314  0.003135  0.007776  0.002812  0.019354   \n",
              "4  0.011491  0.042573  0.011893  0.006483  0.026046  0.033765  0.023007   \n",
              "\n",
              "        95        96        97        98        99        100       101  \\\n",
              "0  0.007550  0.026797  0.052368  0.002064  0.006276  0.019390  0.017358   \n",
              "1  0.032582  0.031485  0.159145  0.028067  0.001149  0.016649  0.043634   \n",
              "2  0.010688  0.011379  0.008183  0.008879  0.005266  0.001854  0.025537   \n",
              "3  0.006797  0.001859  0.004786  0.028971  0.008780  0.004857  0.020632   \n",
              "4  0.002011  0.022124  0.039410  0.018336  0.018446  0.004055  0.011604   \n",
              "\n",
              "        102       103       104       105       106       107       108  \\\n",
              "0  0.029372  0.018410  0.018153  0.020319  0.020459  0.023357  0.006300   \n",
              "1  0.015993  0.031861  0.034243  0.039054  0.033432  0.011109  0.029899   \n",
              "2  0.004010  0.015010  0.019549  0.013936  0.023683  0.000796  0.024786   \n",
              "3  0.018439  0.000859  0.002637  0.022604  0.002541  0.000447  0.009995   \n",
              "4  0.021924  0.004945  0.010201  0.006679  0.012990  0.019164  0.004621   \n",
              "\n",
              "        109       110  \n",
              "0  0.023514  0.012959  \n",
              "1  0.069826  0.028005  \n",
              "2  0.021549  0.004136  \n",
              "3  0.011769  0.007664  \n",
              "4  0.030686  0.009099  \n",
              "\n",
              "[5 rows x 111 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDCfCNgg1JOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upper triangle of correlations\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW9noDjg1JO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select columns with correlations above threshold\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uymmPeTe1JO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_preprocess = X_train_preprocess.drop(columns = to_drop)\n",
        "X_eval_preprocess = X_eval_preprocess.drop(columns = to_drop)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BCBT5AX1JO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_df_preprocess = features_df_preprocess.drop(columns = to_drop)\n",
        "test_features_df_preprocess = test_features_df_preprocess.drop(columns = to_drop)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAEIQFfI2xj_",
        "colab_type": "text"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOdYLy1fn4Kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "est_mlp = MLPClassifier(hidden_layer_sizes=(200,100,), learning_rate='invscaling', learning_rate_init=0.01,\n",
        "                          power_t=0.5, max_iter=500, shuffle=True, tol=0.0001, \n",
        "                          early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, max_fun=15000,\n",
        "                         random_state=RANDOM_SEED)\n",
        "est_xgb = xgb.XGBClassifier(learning_rate =0.03, n_estimators=613, max_depth=5, min_child_weight=3, gamma=0.1,\n",
        "                             subsample=0.9,colsample_bytree=0.45,reg_alpha=0.001,objective= 'binary:logistic',nthread=4,\n",
        "                              scale_pos_weight=1,reg_lambda = 10, seed = 27)\n",
        "est_gb = GradientBoostingClassifier(random_state=8, n_estimators=250)\n",
        "est_rn = RandomForestClassifier(bootstrap=False, max_depth=10, max_features='auto',\n",
        "                       min_samples_leaf=3, min_samples_split=12,\n",
        "                       n_estimators=180,\n",
        "                       n_jobs=None, random_state=None,\n",
        "                       verbose=0, warm_start=False)\n",
        "est_svc = SVC()\n",
        "est_nb = GaussianNB()\n",
        "est_kn = KNeighborsClassifier()\n",
        "est_lgb = lgb.LGBMClassifier(learning_rate=0.03, n_estimators=478, max_depth=5, min_samples_split=100, min_samples_leaf=100, random_state=27, gamma=0.1,\n",
        "                                                       subsample=0.8, reg_alpha = 0.001, reg_lambda=10, objective = 'binary', boosting_type='goss')\n",
        "\n",
        "estimators_stacked = [('xbg', est_xgb),\n",
        "              # ('mlp', est_mlp),\n",
        "              #('gb', est_gb),\n",
        "              # ('rn', est_rn),\n",
        "              ('lgb', est_lgb)\n",
        "             ]\n",
        "\n",
        "model_h1n1 =  StackingClassifier(estimators=estimators_stacked,\n",
        "                                    final_estimator=LogisticRegression(),\n",
        "                                    stack_method='predict_proba'\n",
        "                                  )"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LMka9wDogSw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "929e9e26-4a31-4040-c001-01b843aae965"
      },
      "source": [
        "# train model\n",
        "%%time \n",
        "\n",
        "model_h1n1.fit(X_train_preprocess, y_train)\n",
        "\n",
        "None   # So we don't print out the whole pipeline representation"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 38s, sys: 6.46 s, total: 3min 45s\n",
            "Wall time: 1min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCJ22P1K1JPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on evaluation set\n",
        "# This competition wants probabilities, not labels\n",
        "preds_h1n1 = model_h1n1.predict_proba(X_eval_preprocess)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOGFouVn1JPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5214895d-6536-4349-c299-4c28cd327d38"
      },
      "source": [
        "y_preds_h1n1 = pd.DataFrame(\n",
        "    {\n",
        "        \"h1n1_vaccine\": preds_h1n1[:, 1],\n",
        "        \n",
        "    },\n",
        "    index = y_eval.index\n",
        ")\n",
        "print(\"y_preds_h1n1.shape:\", y_preds_h1n1.shape)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_preds_h1n1.shape: (8814, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke-ZGTR_Nco9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3355a716-1d81-4aee-e8b8-f884fac6e28c"
      },
      "source": [
        "roc_auc_score(y_eval, y_preds_h1n1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8691545005254799"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQb_83i11JPL",
        "colab_type": "text"
      },
      "source": [
        "# Train on whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x3IG9r01JPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "457c575d-1702-435a-8f4f-e0da78e8c6b3"
      },
      "source": [
        "# train model\n",
        "%%time \n",
        "\n",
        "model_h1n1.fit(features_df_preprocess, labels_df_h1n1)\n",
        "\n",
        "None   # So we don't print out the whole pipeline representation"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 23s, sys: 7.31 s, total: 5min 31s\n",
            "Wall time: 2min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L6ETbGD1JPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_probas_h1n1 = model_h1n1.predict_proba(test_features_df_preprocess)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6KqCI9D1JPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18815dff-51ec-4d5b-b00c-143706c41968"
      },
      "source": [
        "y_preds_test_h1n1 = pd.DataFrame(\n",
        "    {\n",
        "        \"h1n1_vaccine\": test_probas_h1n1[:, 1],\n",
        "        \n",
        "    },\n",
        "    index = test_features_df.index\n",
        ")\n",
        "print(\"y_preds_test_h1n1.shape:\", y_preds_test_h1n1.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_preds_test_h1n1.shape: (26708, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbJRQHNx1JPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_test_h1n1.to_csv('my_submission.csv', index=True)"
      ],
      "execution_count": 86,
      "outputs": []
    }
  ]
}